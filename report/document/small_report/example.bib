@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@software{grid2op,
    author = {B. Donnot},
    title = {{Grid2op- A testbed platform to model sequential decision making in power systems. }},
    url = {\url{https://GitHub.com/Grid2Op/grid2op}},
    year = {2020},
    publisher = {GitHub},
}

 
@article{kalidas_deep_2023,
	title = {Deep Reinforcement Learning for Vision-Based Navigation of {UAVs} in Avoiding Stationary and Mobile Obstacles},
	volume = {7},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2504-446X},
	url = {https://www.mdpi.com/2504-446X/7/4/245},
	doi = {10.3390/drones7040245},
	abstract = {Unmanned Aerial Vehicles ({UAVs}), also known as drones, have advanced greatly in recent years. There are many ways in which drones can be used, including transportation, photography, climate monitoring, and disaster relief. The reason for this is their high level of efficiency and safety in all operations. While the design of drones strives for perfection, it is not yet flawless. When it comes to detecting and preventing collisions, drones still face many challenges. In this context, this paper describes a methodology for developing a drone system that operates autonomously without the need for human intervention. This study applies reinforcement learning algorithms to train a drone to avoid obstacles autonomously in discrete and continuous action spaces based solely on image data. The novelty of this study lies in its comprehensive assessment of the advantages, limitations, and future research directions of obstacle detection and avoidance for drones, using different reinforcement learning techniques. This study compares three different reinforcement learning strategies—namely, Deep Q-Networks ({DQN}), Proximal Policy Optimization ({PPO}), and Soft Actor-Critic ({SAC})—that can assist in avoiding obstacles, both stationary and moving; however, these strategies have been more successful in drones. The experiment has been carried out in a virtual environment made available by {AirSim}. Using Unreal Engine 4, the various training and testing scenarios were created for understanding and analyzing the behavior of {RL} algorithms for drones. According to the training results, {SAC} outperformed the other two algorithms. {PPO} was the least successful among the algorithms, indicating that on-policy algorithms are ineffective in extensive 3D environments with dynamic actors. {DQN} and {SAC}, two off-policy algorithms, produced encouraging outcomes. However, due to its constrained discrete action space, {DQN} may not be as advantageous as {SAC} in narrow pathways and twists. Concerning further findings, when it comes to autonomous drones, off-policy algorithms, such as {DQN} and {SAC}, perform more effectively than on-policy algorithms, such as {PPO}. The findings could have practical implications for the development of safer and more efficient drones in the future.},
	pages = {245},
	number = {4},
	journaltitle = {Drones},
	shortjournal = {Drones},
	author = {Kalidas, Amudhini P. and Joshua, Christy Jackson and Md, Abdul Quadir and Basheer, Shakila and Mohan, Senthilkumar and Sakri, Sapiah},
	urldate = {2024-10-21},
	date = {2023-04-01},
	langid = {english},
	file = {Full Text:C\:\\Users\\johan\\Zotero\\storage\\D97UN5IH\\Kalidas et al. - 2023 - Deep Reinforcement Learning for Vision-Based Navigation of UAVs in Avoiding Stationary and Mobile Ob.pdf:application/pdf},
}

@misc{wang_truly_2019,
	title = {Truly Proximal Policy Optimization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1903.07940},
	doi = {10.48550/ARXIV.1903.07940},
	abstract = {Proximal policy optimization ({PPO}) is one of the most successful deep reinforcement-learning methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, its optimization behavior is still far from being fully understood. In this paper, we show that {PPO} could neither strictly restrict the likelihood ratio as it attempts to do nor enforce a well-defined trust region constraint, which means that it may still suffer from the risk of performance instability. To address this issue, we present an enhanced {PPO} method, named Truly {PPO}. Two critical improvements are made in our method: 1) it adopts a new clipping function to support a rollback behavior to restrict the difference between the new policy and the old one; 2) the triggering condition for clipping is replaced with a trust region-based one, such that optimizing the resulted surrogate objective function provides guaranteed monotonic improvement of the ultimate policy performance. It seems, by adhering more truly to making the algorithm proximal - confining the policy within the trust region, the new algorithm improves the original {PPO} on both sample efficiency and performance.},
	publisher = {{arXiv}},
	author = {Wang, Yuhui and He, Hao and Wen, Chao and Tan, Xiaoyang},
	urldate = {2024-10-21},
	date = {2019},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@misc{oh_self-imitation_2018,
	title = {Self-Imitation Learning},
	url = {http://arxiv.org/abs/1806.05635},
	abstract = {This paper proposes Self-Imitation Learning ({SIL}), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that {SIL} significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that {SIL} improves proximal policy optimization ({PPO}) on {MuJoCo} tasks.},
	number = {{arXiv}:1806.05635},
	publisher = {{arXiv}},
	author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
	urldate = {2024-10-21},
	date = {2018-06-14},
	eprinttype = {arxiv},
	eprint = {1806.05635 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\NQU98522\\Oh et al. - 2018 - Self-Imitation Learning.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\SQXW2KH2\\1806.html:text/html},
}

@misc{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	number = {{arXiv}:1707.06347},
	publisher = {{arXiv}},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2024-10-21},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\YWZLVVK4\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\M953NF55\\1707.html:text/html},
}

@misc{pathak_curiosity-driven_2017,
	title = {Curiosity-driven Exploration by Self-supervised Prediction},
	url = {http://arxiv.org/abs/1705.05363},
	abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: {VizDoom} and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
	number = {{arXiv}:1705.05363},
	publisher = {{arXiv}},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	urldate = {2024-10-21},
	date = {2017-05-15},
	eprinttype = {arxiv},
	eprint = {1705.05363 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\WY2TBZTY\\Pathak et al. - 2017 - Curiosity-driven Exploration by Self-supervised Prediction.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\2RRKJQPN\\1705.html:text/html},
}

@misc{wang_dueling_2016,
	title = {Dueling Network Architectures for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.},
	number = {{arXiv}:1511.06581},
	publisher = {{arXiv}},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado van and Lanctot, Marc and Freitas, Nando de},
	urldate = {2024-10-21},
	date = {2016-04-05},
	eprinttype = {arxiv},
	eprint = {1511.06581 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\J7RQP4E8\\Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforcement Learning.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\6SDKRHPN\\1511.html:text/html},
}

@misc{schaul_prioritized_2016,
	title = {Prioritized Experience Replay},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks ({DQN}), a reinforcement learning algorithm that achieved human-level performance across many Atari games. {DQN} with prioritized experience replay achieves a new state-of-the-art, outperforming {DQN} with uniform replay on 41 out of 49 games.},
	number = {{arXiv}:1511.05952},
	publisher = {{arXiv}},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	urldate = {2024-10-21},
	date = {2016-02-25},
	eprinttype = {arxiv},
	eprint = {1511.05952 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\5ZCUQBSU\\Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\UVNREUQC\\1511.html:text/html},
}

@misc{hasselt_deep_2015,
	title = {Deep Reinforcement Learning with Double Q-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	number = {{arXiv}:1509.06461},
	publisher = {{arXiv}},
	author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	urldate = {2024-10-21},
	date = {2015-12-08},
	eprinttype = {arxiv},
	eprint = {1509.06461 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\johan\\Zotero\\storage\\JZEEEGQH\\Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;Snapshot:C\:\\Users\\johan\\Zotero\\storage\\GY25QJ4F\\1509.html:text/html},
}
