%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Research project proposal template
% Based on:
%
% LaTeX Template
% Version 2.5 (27/8/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Version 2.x major modifications by: 
% Helen Robertson
%
% With thanks to:
% Matthew Woolway and Terence Van Zyl for help with coding and content.
%
% This template is based on a template by:
% Steve Gunn (http://users.ecs.soton.ac.uk/srg/softwaretools/document/templates/)
% Sunil Patel (http://www.sunilpatel.co.uk/thesis-template/)
%
% Template license:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% This template has been constructed in accordance with the requirements and conventions of the School of Computer Science and Applied Mathematics and of the Faculty of Science at the University of the Witwatersrand.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
12pt, % The default document font size, options: 10pt, 11pt, 12pt
oneside, % Two side (alternating margins) for binding by default, uncomment to switch to one side
english, % ngerman for German
onehalfspacing, % One-and-a-half line spacing, alternatives: singlespacing or doublespacing
%draft, % Uncomment to enable draft mode (no pictures, no links, overfull hboxes indicated)
nolistspacing, % If the document is onehalfspacing or doublespacing, uncomment this to set spacing in lists to single
liststotoc, % Uncomment to add the list of figures/tables/etc to the table of contents
%toctotoc, % Uncomment to add the main table of contents to the table of contents
%parskip, % Uncomment to add space between paragraphs
%nohyperref, % Uncomment to not load the hyperref package
headsepline, % Uncomment to get a line under the header
%chapterinoneline, % Uncomment to place the chapter title next to the number on one line
%consistentlayout, % Uncomment to change the layout of the declaration, abstract and acknowledgements pages to match the default layout
]{ProposalAndThesis} % The class file specifying the document structure

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters

\usepackage{mathpazo} % Use the Palatino font by default

\usepackage[backend=bibtex,style=numeric,natbib=true]{biblatex} % Use the bibtex backend with the numeric citation style

\addbibresource{example.bib} % The filename of the bibliography

\usepackage[autostyle=true]{csquotes} % Required to generate language-dependent quotes in the bibliography

\usepackage[cleanlook, english]{isodate} % Required for UK date formatting

\usepackage{fancybox} % Required for boxed text sections
\usepackage{xcolor} % Required for coloured text

\usepackage{pgfgantt}

\definecolor{ShadowColor}{RGB}{0,103,165} % Required for coloured shadow in boxed text sections
\makeatletter
\newcommand\Cshadowbox{\VerbBox\@Cshadowbox}
\def\@Cshadowbox#1{%
	\setbox\@fancybox\hbox{\fbox{#1}}%
	\leavevmode\vbox{%
		\offinterlineskip
		\dimen@=\shadowsize
		\advance\dimen@ .5\fboxrule
		\hbox{\copy\@fancybox\kern.5\fboxrule\lower\shadowsize\hbox{%
				\color{ShadowColor}\vrule \@height\ht\@fancybox \@depth\dp\@fancybox \@width\dimen@}}%
		\vskip\dimexpr-\dimen@+0.5\fboxrule\relax
		\moveright\shadowsize\vbox{%
			\color{ShadowColor}\hrule \@width\wd\@fancybox \@height\dimen@}}}
\makeatother

%----------------------------------------------------------------------------------------
%	MARGIN SETTINGS
%----------------------------------------------------------------------------------------

\geometry{
	paper=a4paper, % Change to letterpaper for US letter
	inner=2.8cm, % Inner margin
	outer=2.8cm, % Outer margin
	bindingoffset=0.0cm, % Binding offset
	top=2.8cm, % Top margin
	bottom=2.8cm, % Bottom margin
	%showframe, % Uncomment to show how the type block is set on the page
}

%----------------------------------------------------------------------------------------
%	THESIS INFORMATION
%----------------------------------------------------------------------------------------

\thesistitle{COMS7071A Assignment: Grid2Op using DQN and PPO} % Your thesis title, this is used in the title and abstract, print it elsewhere with \ttitle
% \supervisor{Name of Supervisor Here} % Your supervisor's name, this is used in the title page, print it elsewhere with \supname
% \cosupervisor{Name of Co-supervisor Here (or Delete)} % Your co-supervisor's name, this is used in the title page, print it elsewhere with \cosupname
%\examiner{} % Your examiner's name, this is not currently used anywhere in the template, print it elsewhere with \examname
% \degree{Degree Name Here} % Your degree name, this is used in the title page and abstract, print it elsewhere with \degreename
\author{Willem Van Der Merwe 2914429} % Your name, this is used in the title page and abstract, print it elsewhere with \authorname
%\addresses{} % Your address, this is not currently used anywhere in the template, print it elsewhere with \addressname
%\subject{} % Your subject area, this is not currently used anywhere in the template, print it elsewhere with \subjectname
%\keywords{} % Keywords for your thesis, this is not currently used anywhere in the template, print it elsewhere with \keywordnames
\university{University of the Witwatersrand, Johannesburg} % Your university's name, this is used in the title page and abstract, print it elsewhere with \univname
% \department{Name of School or Department Here} % Your department's name, this is used in the title page and abstract, print it elsewhere with \deptname
%\group{\href{http://research group.com}{Research Group Name}} % Your research group's name and URL, this is not currently used anywhere in the template, print it elsewhere with \groupname
%\faculty{\href{http://faculty.university.com}{Faculty Name}} % Your faculty's name and URL, this is not currently used anywhere in the template, print it elsewhere with \facname

\def\keywordnames{Appendices; Chapters; Figures; example.bib; main.pdf; main.tex; main.bbl; main.aux; main.blg; main.lof; main.log; main.lot; main.out; MastersDoctoralThesis.cls}

\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}



\AtBeginDocument{
\hypersetup{pdftitle=\ttitle} % Set the PDF's title to your title
\hypersetup{pdfauthor=\authorname} % Set the PDF's author to your name
\hypersetup{pdfkeywords=\keywordnames} % Set the PDF's keywords to your keywords
}

\begin{document}

\frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the pre-content pages

\pagestyle{plain} % Default to the plain heading style until the thesis style is called for the body content

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}
\begin{center}

{\huge \bfseries \ttitle}\par\vspace{0.4cm} % Thesis title
\HRule\par\vspace{1.5cm}
\authorname\par\vspace{1cm}
% \emph{Supervisor(s):}\par
% {\supname}\par % Name of supervisor
% {\cosupname} % Name of co-supervisor
\par\vspace{0.5cm}

\includegraphics[width=80mm]{Figures/logoWitsstackedcolourtransparent.png} % University crest
\vfill

% A research proposal submitted in partial fulfillment of the requirements for the degree of \par\vspace{0.3cm}
% in the\par\vspace{0.4cm}
% \deptname\par\vspace{0.1cm} % Name of department
\univname\par\vspace{0.4cm} % Name of university
\cleanlookdateon
\today % Date

\end{center}

\end{titlepage}

%---------------------------------------------------------------------------------------
%	USING THIS TEMPLATE
%---------------------------------------------------------------------------------------

% \par\vspace{0.5cm}
% \noindent \Cshadowbox{
% 	\begin{minipage}{15cm}
% 		\medskip
% 		\color[rgb]{0.0,0.4,0.65} 
% 		\begin{center} 
% 			\medskip \textbf{Using this template} \par 
% 			\end{center}
% 			\medskip The template below has been constructed in line with the Faculty of Science requirements as well as the conventions of the School. Guidance on each section can be found in the blue boxes at the start of the section. Note that conventions and preferences for structuring a research proposal can vary from discipline to discipline and supervisor to supervisor. Both the template and the guidance on the different sections are suggestions for structuring the proposal. If your supervisor has a different preferred convention or template, it is recommended that you consult with them regarding the differences. \par \medskip
% 			Further details on using the template can be found in Appendix B. \par \medskip
% 			You should ensure that you either delete or comment out the blue boxes before submission of the proposal. \par
% 		\medskip 
% \end{minipage}}\\ 

%----------------------------------------------------------------------------------------
%	DECLARATION PAGE
%----------------------------------------------------------------------------------------

% \begin{declaration}
% \addchaptertocentry{\authorshipname} % Add the declaration to the table of contents
% \vspace{0.5cm}
% \noindent I, \authorname, declare that this proposal is my own, unaided work. It is being submitted for the degree of {\degreename} at the \univname. It has not been submitted for any degree or examination at any other university.

% \par\vspace{2cm}
% \begin{flushright}
% \includegraphics[width=30mm]{Figures/sign.png}\par 
% \authorname\par\vspace{0.1cm}
% \today
% \end{flushright}

% \par\vspace{0.5cm}
% \noindent \Cshadowbox{
% 	\begin{minipage}{15cm}
% 		\medskip
% 		\color[rgb]{0.0,0.4,0.65} 
% 		\medskip The declaration is an important formal requirement. Ensure that you upload an
% 		image of your signature and that you change the file name in the main.tex file to include it.
% 		\medskip 
% \end{minipage}}\\ 

% \end{declaration}
% \vfill
\pagebreak

%----------------------------------------------------------------------------------------
%	QUOTATION PAGE
%----------------------------------------------------------------------------------------

%\vspace*{0.2\textheight}

%\noindent\enquote{\itshape Thanks to my solid academic training, today I can write hundreds of words on virtually any topic without possessing a shred of information, which is how I got a good job in journalism.}\bigbreak

%\hfill Dave Barry

%----------------------------------------------------------------------------------------
%	ABSTRACT PAGE
%----------------------------------------------------------------------------------------

% \begin{abstract}
% \addchaptertocentry{\abstractname} % Add the abstract to the table of contents
% \begin{quote}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam ultricies lacinia euismod. Nam tempus risus in dolor rhoncus in interdum enim tincidunt. Donec vel nunc neque. In condimentum ullamcorper quam non consequat. Fusce sagittis tempor feugiat. Fusce magna erat, molestie eu convallis ut, tempus sed arcu. Quisque molestie, ante a tincidunt ullamcorper, sapien enim dignissim lacus, in semper nibh erat lobortis purus. Integer dapibus ligula ac risus convallis pellentesque.
% \end{quote}

% \par\vspace{0.5cm}
% \noindent \Cshadowbox{
%     \begin{minipage}{15cm}
%     \bigskip
%     \color[rgb]{0.0,0.4,0.65}The abstract is a brief informative summary of the proposed research. It can be read independently and should 
%     	\begin{itemize}
%     		\item locate the proposed research within the background relevant to it,
%     		\item state the research question or aim,
%     		\item briefly describe the proposed methods for answering the question or achieving the aim, and,
%     		\item emphasise the contribution that the proposed research will make to current research in the field.
%     	\end{itemize}
%     	It is recommended that your abstract is no more than 300 words.
%     \medskip
% 	\end{minipage}}

% \end{abstract}

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

% \begin{acknowledgements}
% \addchaptertocentry{\acknowledgementname} % Add the acknowledgements to the table of contents
% \vspace{0.5cm}
% \noindent{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam ultricies lacinia euismod. Nam tempus risus in dolor rhoncus in interdum enim tincidunt. Donec vel nunc neque.}

% \par\vspace{0.5cm}
% \noindent \Cshadowbox{
% 	\begin{minipage}{15cm}
% 		\medskip
% 		\color[rgb]{0.0,0.4,0.65} 
% 		\medskip The acknowledgements section allows you to thank those who contributed to the
% 		preparation of the proposal. It is usual to acknowledge supervision, financial
% 		assistance or funders, and any special facilities provided for the research.		
% 		\medskip 
% \end{minipage}}\\ 



% \end{acknowledgements}

%----------------------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES PAGES
%----------------------------------------------------------------------------------------

% \tableofcontents % Prints the main table of contents

% \listoffigures % Prints the list of figures

% \listoftables % Prints the list of tables

%----------------------------------------------------------------------------------------
%	ABBREVIATIONS
%----------------------------------------------------------------------------------------

%\begin{abbreviations}{ll} % Include a list of abbreviations (a table of two columns)

%\textbf{LAH} & \textbf{L}ist \textbf{A}bbreviations \textbf{H}ere\\
%\textbf{WSF} & \textbf{W}hat (it) \textbf{S}tands \textbf{F}or\\

%\end{abbreviations}

%----------------------------------------------------------------------------------------
%	PHYSICAL CONSTANTS/OTHER DEFINITIONS
%----------------------------------------------------------------------------------------

%\begin{constants}{lr@{${}={}$}l} % The list of physical constants is a three column table

% The \SI{}{} command is provided by the siunitx package, see its documentation for instructions on how to use it

%Speed of Light & $c_{0}$ & \SI{2.99792458e8}{\meter\per\second} (exact)\\
%Constant Name & $Symbol$ & $Constant Value$ with units\\

%\end{constants}

%----------------------------------------------------------------------------------------
%	SYMBOLS
%----------------------------------------------------------------------------------------

%\begin{symbols}{lll} % Include a list of Symbols (a three column table)

%$a$ & distance & \si{\meter} \\
%$P$ & power & \si{\watt} (\si{\joule\per\second}) \\
%Symbol & Name & Unit \\

%\addlinespace % Gap to separate the Roman symbols from the Greek

%$\omega$ & angular frequency & \si{\radian} \\

%\end{symbols}

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

%\dedicatory{For/Dedicated to/To my\ldots} 

%----------------------------------------------------------------------------------------
%	THESIS CONTENT - CHAPTERS
%----------------------------------------------------------------------------------------

\mainmatter % Begin numeric (1,2,3...) page numbering

%\pagestyle{thesis} % Return the page headers back to the "thesis" style

% Include the chapters of the thesis as separate files from the Chapters folder
% Uncomment the lines as you write the chapters


%----------------------------------------------------------------------------------------
%	THESIS CONTENT - APPENDICES
%----------------------------------------------------------------------------------------

% \appendix % Cue to tell LaTeX that the following "chapters" are Appendices

% Include the appendices of the thesis as separate files from the Appendices folder
% Uncomment the lines as you write the Appendices

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% \printbibliography[heading=bibintoc]

%----------------------------------------------------------------------------------------




\chapter{Model Architectures}

The reinforcement learning algorithms selected for this study are Proximal Policy Optimization (PPO) and Deep Q-Network (DQN). These algorithms will be implemented and iteratively improved to optimize power grid operations in the Grid2Op environment.

\section{Deep Q-Network (DQN)}
DQN is a value-based method that approximates the optimal action-value function using deep neural networks, which is suitable for handling high-dimensional state spaces. DQN relies on a Q-learning framework combined with a deep neural network to approximate the optimal Q-values for each state-action pair, which enables the agent to make decisions that maximize expected rewards. Due to this algorithm already being part of prior assessments I don't cover the architecture indetail.

\section{Proximal Policy Optimization (PPO)}
PPO is a policy gradient method designed to optimize a parameterized policy by balancing exploration and exploitation while avoiding large, destabilizing policy updates. PPO's stability and efficiency stem from its use of a clipped surrogate objective function to limit the extent of each policy update, ensuring that learning is reliable without excessive policy divergence.

PPO belongs to the category of policy optimization methods and utilizes an Actor-Critic architecture, which involves updating both the policy (actor) and the value function (critic). The primary goal of PPO is to constrain policy updates to prevent large steps that can destabilize learning. Unlike traditional policy gradient methods, PPO uses a clipped surrogate objective to regulate policy updates, thus ensuring stable and reliable training. Below, the structure and workings of PPO are detailed:

\paragraph{Actor-Critic Architecture}: PPO uses an actor network, which outputs a probability distribution over actions, and a critic network, which estimates the value function—the expected return from a given state. The actor explores the action space by selecting actions probabilistically, while the critic provides feedback on the quality of the selected actions, thereby guiding the actor's learning.

\paragraph{Clipped Surrogate Objective}: PPO maintains stability through the use of a clipped surrogate objective function. The objective function for PPO is given by:
  
  
  \[ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right] \]

  Here, \( r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \) represents the ratio of the probability of selecting action \( a_t \) under the updated policy to the probability under the old policy, and \( \hat{A}_t \) represents the advantage function. The advantage function quantifies how much better the chosen action is compared to the average action for the given state. The clipping operation constrains \( r_t(\theta) \) to lie within the interval \([1 - \epsilon, 1 + \epsilon]\), preventing updates that are excessively large or too small, thus keeping learning within a stable range.

\paragraph{Advantage Estimation}: PPO employs Generalized Advantage Estimation (GAE) to compute the advantage function \( \hat{A}_t \). GAE is useful for reducing the variance in policy gradient estimates while preserving their accuracy, leading to more stable and efficient learning.

  
  \[ \hat{A}_t = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l} \]
  
  Where \( \gamma \) is the discount factor, and \( \lambda \) controls the bias-variance tradeoff. The temporal difference (TD) error \( \delta_t \) measures how well the value function estimates the return compared to the observed outcome.

\paragraph{Entropy Regularization}: Entropy regularization is employed in PPO to ensure that the policy maintains adequate exploration throughout the learning process. By adding an entropy term to the objective function, PPO encourages the policy to remain diverse in its action selection, thus preventing premature convergence to suboptimal strategies.

\paragraph{Policy Updates}: The policy is updated using stochastic gradient ascent, aiming to maximize the clipped surrogate objective. To ensure stable and gradual learning, PPO performs multiple epochs of minibatch updates with a small learning rate. This process ensures that the policy does not change too drastically in any one update.

\paragraph{Implementation Details}: During training, PPO alternates between collecting data by running the current policy in the environment and optimizing the clipped surrogate objective using the collected data. Key hyperparameters include the clipping threshold \( \epsilon \), the discount factor \( \gamma \), the GAE parameter \( \lambda \), and the number of epochs per iteration. Proper tuning of these hyperparameters is critical to the success of PPO in achieving stable learning.



\chapter{Model Improvements architectures}
The methodology involves an iterative improvement process for both PPO and DQN. Initially, a base version of each algorithm will be implemented. Subsequently, three iterations of improvements will be applied to both agents. Each improvement is designed to incrementally enhance agent performance and robustness, allowing for an in-depth analysis of the impact of each enhancement.

\section{DQN Improvements}

\paragraph{Iteration 1} Implement Double DQN to mitigate overestimation bias in action-value estimation, leading to more accurate Q-value predictions and enhanced policy quality.

Double DQN addresses a fundamental limitation of the original DQN: overestimation bias in action-value predictions. In traditional DQN, the same network is used for both action selection and evaluation, which tends to result in overly optimistic value estimates. Double DQN resolves this by using two separate networks—one to select actions and the other to evaluate their value.

The key difference lies in how the target values are calculated. Instead of using the maximum Q-value estimated by the same network, Double DQN uses the main network to select the action and the target network to evaluate its value. This decoupling of action selection from action evaluation reduces the upward bias and results in more accurate value estimates.

The implementation of Double DQN involves maintaining two neural networks: the primary Q-network and the target Q-network. The target network is updated periodically, providing a stable reference point for value calculations, while the main network is updated at each step. This helps maintain stability in the learning process by preventing rapid shifts in target estimates.

The impact of Double DQN on training output is significant, as it reduces the likelihood of divergence and leads to more reliable policy learning. The reduced bias in value estimation contributes to smoother convergence and a more stable policy, which is particularly advantageous in complex environments like power grid management.

\paragraph{Iteration 2}: Implement Dueling DQN to differentiate between the value of being in a state and the advantage of selecting specific actions, thereby enhancing learning efficiency.

Dueling DQN introduces a novel architecture that separates the estimation of the state-value function from the advantage function. The state-value function estimates the value of being in a particular state, while the advantage function estimates the relative benefit of each possible action compared to an average action in that state.

The key concept is that not all actions are equally important in every state. In many situations, the value of a state is independent of the specific action taken. By having separate streams for state-value and advantage estimation, Dueling DQN allows the network to focus on learning the value of the state itself without being overly influenced by the choice of action.

Implementation involves modifying the network architecture to create two separate output streams: one for the state-value and one for the advantage function. These streams are then combined to compute the Q-values for each action. This architectural change allows the network to better differentiate between states that are generally good and actions that provide additional benefits, which accelerates the learning process.

The output of Dueling DQN is a more efficient learning process with faster convergence. The separation of value and advantage leads to improved stability, particularly in environments where many actions yield similar outcomes. This allows the agent to better generalize across states and actions, resulting in more robust policies.

\paragraph{Iteration 3}: Integrate Prioritized Experience Replay (PER) to improve learning efficiency by sampling important transitions more frequently, thereby focusing updates on the most informative experiences.

    Prioritized Experience Replay (PER) enhances the efficiency of experience replay by assigning a priority value to each transition in the replay buffer, based on its temporal difference (TD) error. Transitions with larger TD errors indicate greater learning potential, as they represent situations where the agent's prediction was significantly incorrect.

    The core idea behind PER is to focus the learning process on the experiences that will have the greatest impact on improving the policy. By prioritizing transitions with high TD errors, the agent can more effectively correct its mistakes, leading to faster learning and improved policy quality.

    Implementation of PER requires modifying the replay buffer to maintain priority values for each transition. During sampling, transitions with higher priority are more likely to be replayed, ensuring that the agent focuses on learning from its most informative experiences. Additionally, importance sampling weights are used to correct any bias introduced by the prioritized sampling process, ensuring stable updates.

    The impact of PER on output is significant, as it accelerates the convergence of the learning process. By focusing on key experiences, the agent is able to learn more effectively from limited data, which is particularly advantageous in environments with sparse rewards. This leads to more effective policy updates, reduced training time, and better overall performance.

\section{PPO Improvements}

\paragraph{Iteration 1}:
    
Implement the "Truly PPO" methodology as described in the 2020 paper by Yuhui Wang, Hao He, and Xiaoyang Tan. This iteration involves introducing a more adaptive approach to clipping, refined hyperparameter tuning, and improved balance between exploration and exploitation.

The "Truly PPO" version aims to address limitations of the original PPO by enhancing the adaptability of the clipping mechanism. The clipping threshold is dynamic rather than fixed, enabling policy updates that are more context-sensitive. This adaptability results in more reliable learning, particularly in environments with diverse state distributions.

The core enhancement lies in how the surrogate objective is managed. By making the clipping region adaptive, the agent can maintain a balance between stability and the capacity for significant learning updates. This modification allows the policy to evolve naturally, without the risk of becoming overly constrained or excessively volatile.

Implementing "Truly PPO" necessitates careful hyperparameter adjustment to ensure optimal clipping behavior, which introduces computational overhead but yields significant improvements in policy stability and convergence. Extensive evaluation is needed to understand the impact of these adjustments across various phases of the training cycle.

Overall, "Truly PPO" provides a framework that facilitates smoother, more stable updates, contributing to enhanced reward maximization over the long term.

\paragraph{Iteration 2}:
    
Incorporate the Intrinsic Curiosity Module (ICM) to enhance exploration by introducing a curiosity-based reward signal. ICM models the prediction error of transitioning to new states, thereby fostering exploration in less-visited areas of the state space.

The Intrinsic Curiosity Module augments the standard reward signal with an intrinsic reward based on prediction error. The ICM consists of a forward model and an inverse model. The forward model predicts the next state given the current state and action, while the inverse model predicts the action given consecutive states. The magnitude of the prediction error from the forward model serves as the intrinsic reward, which encourages the agent to visit states where it has a high level of uncertainty.

The primary goal of ICM is to motivate exploration, particularly in environments where external rewards are sparse or delayed. This added incentive helps the agent discover new strategies that could yield better results. In power grid operations, ICM might encourage the agent to explore a wider range of control actions, which could lead to more robust grid management strategies.

From an implementation perspective, integrating ICM requires modifying the neural network architecture to include the forward and inverse models, increasing computational complexity and training requirements. However, this added complexity is justified by the resulting improvements in exploration efficiency, leading to more robust policy performance.

By encouraging exploration through intrinsic motivation, ICM helps mitigate the problem of premature convergence and ensures a more exhaustive search of the state-action space, ultimately resulting in a better-performing policy.

\paragraph{Note}: No improvement 3 for ppo. A Candidate improvement was the novel ...... self imitation learning adaptation, the source code is included in the submission however results and runtimes not covered in the report.

\chapter{Runtime And Configuration}

\section{Environment Setup}
The Grid2Op environment will be used to simulate power grid operations. Specifically, the "l2rpn\_case14\_sandbox" environment is chosen for its simplicity and focus on power flow management without adversarial components. The observation and action spaces will be adapted to suit both PPO and DQN agents, ensuring that the agents can effectively interact with the environment.
The reward function used in this study is a **CombinedScaledReward**, which is a weighted combination of the L2RPN Reward and the N-1 Reward. This combined reward encourages the agents to maximize power flow while maintaining grid resilience by ensuring there are no single points of failure.

\section{Train Time}
\section{Inference}

\section{Hyperparameters}

\chapter{Evaluations}

Each iteration will follow the same structure for evaluation:

- **Iteration 1**: Evaluate the base agent's performance and analyze the impact of the first improvement.
- **Iteration 2**: Assess the agent after the next improvement and compare against previous results.
- **Iteration 3**: Final evaluation after the last improvement, summarizing overall progress and effectiveness.



\end{document}  
